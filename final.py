# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YBy0N396d_8aokNzQSUbVHd91R7iADgr
"""

pip install --upgrade scikit-learn

import numpy as np
import pandas as pd
import datetime as dt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/Prices for Food Crops.csv')

df.head()

df.drop(columns=['Unnamed: 0'], inplace=True)
df.drop(columns=['Values_in_Ksh'], inplace=True)
df.drop(columns=['Type_of_Commodity'], inplace=True)
df.drop(columns=['produce_variety'], inplace=True)
df.drop(columns=['Volume_in_Kgs'], inplace=True)
df.drop(columns=['Package_Type'], inplace=True)

#  creating a new dataframe from the columns of df with just columns with 'commodity type' and 'product variety'
#df_new = df[['Commodity_Type', '']].copy()
#df.drop(columns=['Commodity_Type', 'Produce_Variety'], inplace=True)

df.head()

df.info()

df.describe()

import matplotlib.pyplot as plt
df.hist(bins=50, figsize=(20,15))
plt.show()

df

"""Encode categorical variables"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    df[col] = label_encoder.fit_transform(df[col])

df                ##encoded

corr_matrix = df.corr()
corr_matrix

"""separate features and target"""

X = df.drop('Price', axis=1)
y = df['Price']

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)

Xtrain.drop(columns=['Date'], inplace=True)
Xtest.drop(columns=['Date'], inplace=True)

Xtrain

ytrain

from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# Create copies of the original data
Xtrain_processed = Xtrain.copy()
Xtest_processed = Xtest.copy()

# Identify columns with datetime data type
datetime_cols = Xtrain_processed.select_dtypes(include=['datetime64']).columns

# Convert datetime columns to numerical representation (Unix timestamp)
for col in datetime_cols:
    Xtrain_processed[col] = pd.to_datetime(Xtrain_processed[col]).astype('int64') // 10**9
    Xtest_processed[col] = pd.to_datetime(Xtest_processed[col]).astype('int64') // 10**9

# Scaling the features
scaler_X = StandardScaler()
Xtrain_scaled = scaler_X.fit_transform(Xtrain_processed)
Xtest_scaled = scaler_X.transform(Xtest_processed)

# Scaling the target variable
scaler_y = StandardScaler()
ytrain_scaled = scaler_y.fit_transform(ytrain.values.reshape(-1, 1))
ytest_scaled = scaler_y.transform(ytest.values.reshape(-1, 1))

"""base models: Random Forest Regressor"""

#we are using random forest regressor as our base model
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


base_model = RandomForestRegressor(n_estimators=100)
base_model.fit(Xtrain_scaled, ytrain_scaled.ravel())

lm = LinearRegression()
lm.fit(Xtrain_scaled, ytrain_scaled)

dt = DecisionTreeRegressor()
dt.fit(Xtrain_scaled, ytrain_scaled)

#predict using the base models
y_pred = base_model.predict(Xtest_scaled)

#transforming the scaled prediction back to its original unit
y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))

lm_pred = lm.predict(Xtest_scaled)
lm_pred_original = scaler_y.inverse_transform(lm_pred.reshape(-1, 1))

dt_pred = dt.predict(Xtest_scaled)
dt_pred_original = scaler_y.inverse_transform(dt_pred.reshape(-1, 1))

y_pred

"""evaluating using default metric"""

base_model.score(Xtest_scaled,ytest_scaled)

lm.score(Xtest_scaled,ytest_scaled)

dt.score(Xtest_scaled,ytest_scaled)

"""evaluation"""

#evaluating the model
rmse = np.sqrt(mean_squared_error(ytest, y_pred))
print('Random Forest Regressor')
print(f'Base Model RMSE: {rmse}')

mse = mean_squared_error(ytest, y_pred)
print(f'Base Model MSE: {mse}')

mae = mean_absolute_error(ytest, y_pred)
print(f'Base Model MAE: {mae}')

r2 = r2_score(ytest, y_pred)
print(f'Base Model R²: {r2}')

rmse = np.sqrt(mean_squared_error(ytest, lm_pred_original))
print('Linear Regression')
print(f'Base Model RMSE: {rmse}')

#mse = mean_squared_error(ytest, lm_pred_original)
#print(f'Base Model MSE: {mse}')

mae = mean_absolute_error(ytest, lm_pred_original)
print(f'Base Model MAE: {mae}')

r2 = r2_score(ytest, lm_pred_original)
print(f'Base Model R²: {r2}')

rmse = np.sqrt(mean_squared_error(ytest, dt_pred_original))
print('Decision Tree Regressor')
print(f'Base Model RMSE: {rmse}')

#mse = mean_squared_error(ytest, dt_pred_original)
#print(f'Base Model MSE: {mse}')

mae = mean_absolute_error(ytest, dt_pred_original)
print(f'Base Model MAE: {mae}')

r2 = r2_score(ytest, dt_pred_original)
print(f'Base Model R²: {r2}')

"""Actual price vs Predicted price"""

# Plotting actual vs predicted prices
plt.figure(figsize=(10, 5))
plt.scatter(ytest, y_pred_original, alpha=0.5)
plt.plot([min(ytest), max(ytest)], [min(ytest), max(ytest)], 'r')
plt.title('Actual vs Predicted Prices (Random Tree Regessor)')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.show()

plt.figure(figsize=(10,5))
plt.scatter(ytest, lm_pred_original, alpha=0.5)
plt.plot([min(ytest), max(ytest)], [min(ytest), max(ytest)], 'r')
plt.title('Actual vs Predicted Prices (Linear Regression)')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.show()

plt.figure(figsize = (10,5))
plt.scatter(ytest, dt_pred_original, alpha=0.5)
plt.plot([min(ytest), max(ytest)], [min(ytest), max(ytest)], 'r')
plt.title('Actual vs Predicted Prices (Decision Tree Regressor)')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.show()

print(f'ytest: {ytest}')
#print(f'y_pred_original: {y_pred_original}')

"""# Hyperparameter tuning"""

# Initialize models
#linear = LinearRegression()
#decisionTree = DecisionTreeRegressor()
#randomForest = RandomForestRegressor()

# Hyperparameter tuning using GridSearchCV for Linear Regression
param_grid_linear = {
    'fit_intercept': [True, False],
    #'normalize': [True, False]
}
grid_search = GridSearchCV(estimator=lm, param_grid=param_grid_linear, cv=5, scoring='r2', n_jobs=-1, verbose=2)
grid_search.fit(Xtrain_scaled, ytrain_scaled.ravel())
best_lnr = grid_search.best_estimator_
best_lnr

# Hyperparameter tuning using GridSearchCV for Decision Tree
param_grid_Dtree = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid_Dtree, cv=5, scoring='r2', n_jobs=-1, verbose=2)
grid_search.fit(Xtrain_scaled, ytrain_scaled.ravel())
best_Dtree = grid_search.best_estimator_
best_Dtree

# Hyperparameter tuning using GridSearchCV for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=base_model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=2)
grid_search.fit(Xtrain_scaled, ytrain_scaled.ravel())
best_rf = grid_search.best_estimator_
best_rf

# Train the models
best_lnr.fit(Xtrain_scaled, ytrain_scaled.ravel())
best_Dtree.fit(Xtrain_scaled, ytrain_scaled.ravel())
best_rf.fit(Xtrain_scaled, ytrain_scaled.ravel())


# Evaluate the models
models = {'Linear Regression': best_lnr, 'Decision Tree': best_Dtree, 'Random Forest': best_rf}

for name, model in models.items():
    y_pred = model.predict(Xtest_scaled) #ytest, y_pred_original
    print(f"Model: {name}")
    print(f"Model Accuracy: {model.score(Xtest_scaled, ytest_scaled)}")
    print(f"MAE: {mean_absolute_error(ytest, y_pred)}")
    print(f"MSE: {mean_squared_error(ytest, y_pred)}")
    print(f"RMSE: {np.sqrt(mean_squared_error(ytest, y_pred))}")
    print(f"R²: {r2_score(ytest, y_pred)}\n")

best_rf

feature_importances = pd.Series(best_rf.feature_importances_, index=Xtrain.columns).sort_values(ascending=False)
print(feature_importances)

from sklearn.model_selection import cross_val_score
from sklearn.inspection import permutation_importance

# Cross-validation
cv_scores = cross_val_score(best_rf, Xtrain_scaled, ytrain_scaled.ravel(), cv=5)
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean CV score: {cv_scores.mean()}")

feature_names = Xtrain.columns.tolist()

# Feature importance
result = permutation_importance(best_rf, Xtest_scaled, ytest_scaled, n_repeats=10, random_state=42)
for i in result.importances_mean.argsort()[::-1]:
    print(f"{feature_names[i]}: {result.importances_mean[i]:.3f} +/- {result.importances_std[i]:.3f}")

feature_names

"""# Passing the Base Models through the Ensemble Learning models."""

from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor

# Gradient Boosting
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(Xtrain_scaled, ytrain_scaled.ravel())
gb_predictions = gb_model.predict(Xtest_scaled)

print("Gradient Boosting Model Performance:")
print(f"Accuracy: {gb_model.score(Xtest_scaled, ytest_scaled)}")
print(f"MAE: {mean_absolute_error(ytest, gb_predictions)}")
print(f"RMSE: {np.sqrt(mean_squared_error(ytest, gb_predictions))}")
print(f"R²: {r2_score(ytest, gb_predictions)}")

# XGBoost
xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(Xtrain_scaled, ytrain_scaled.ravel())
xgb_predictions = xgb_model.predict(Xtest_scaled)

print("\nXGBoost Model Performance:")
print(f"Accuracy: {xgb_model.score(Xtest_scaled, ytest_scaled)}")
print(f"MAE: {mean_absolute_error(ytest, xgb_predictions)}")
print(f"RMSE: {np.sqrt(mean_squared_error(ytest, xgb_predictions))}")
print(f"R²: {r2_score(ytest, xgb_predictions)}")

from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge

# Define base models
base_models = [
    ('lr', lm),
    ('dt', dt),
    ('rf', best_rf)
]

# Define meta-model
meta_model = Ridge()

# Create stacking ensemble
stacking_model = StackingRegressor(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5  # Number of folds for cross-validated predictions
)

# Train the stacking ensemble
stacking_model.fit(Xtrain_scaled, ytrain_scaled.ravel())

# Make predictions
stacking_predictions = stacking_model.predict(Xtest_scaled)

# Evaluate the stacking ensemble
print("Stacking Ensemble Model Performance:")
print(f"Accuracy: {stacking_model.score(Xtest_scaled, ytest_scaled)}")
print(f"MAE: {mean_absolute_error(ytest, stacking_predictions)}")
print(f"MSE: {mean_squared_error(ytest, stacking_predictions)}")
print(f"RMSE: {np.sqrt(mean_squared_error(ytest, stacking_predictions))}")
print(f"R²: {r2_score(ytest, stacking_predictions)}")

from sklearn.ensemble import VotingRegressor

#assuming you have already trained linear, decisionTree, and best_rf models
ensemble_model = VotingRegressor(
    estimators=[
        ('lr', lm),
        ('dt', dt),
        ('rf', best_rf)
    ]
)

# Train the ensemble
ensemble_model.fit(Xtrain_scaled, ytrain_scaled.ravel())

# Make predictions
ensemble_predictions = ensemble_model.predict(Xtest_scaled)

# Evaluate the ensemble
print("Ensemble Model Performance:")
print(f"Accuracy: {ensemble_model.score(Xtest_scaled, ytest_scaled)}")
print(f"MAE: {mean_absolute_error(ytest, ensemble_predictions)}")
print(f"MSE: {mean_squared_error(ytest, ensemble_predictions)}")
print(f"RMSE: {np.sqrt(mean_squared_error(ytest, ensemble_predictions))}")
print(f"R²: {r2_score(ytest, ensemble_predictions)}")

def weighted_average_ensemble(models, weights, X):
    predictions = np.column_stack([model.predict(X) for model in models])
    return np.average(predictions, axis=1, weights=weights)

#assign weights based on R² scores (you might want to use a different metric)
total_r2 = sum([r2_score(ytest, model.predict(Xtest_scaled)) for model in models.values()])
weights = [r2_score(ytest, model.predict(Xtest_scaled)) / total_r2 for model in models.values()]

# Make predictions
weighted_predictions = weighted_average_ensemble(list(models.values()), weights, Xtest_scaled)

print("Weighted Average Ensemble Performance:")
print(f"MAE: {mean_absolute_error(ytest, weighted_predictions)}")
print(f"MSE: {mean_squared_error(ytest, weighted_predictions)}")
print(f"RMSE: {np.sqrt(mean_squared_error(ytest, weighted_predictions))}")
print(f"R²: {r2_score(ytest, weighted_predictions)}")

"""# building a deep learning model

Long-short term memory
"""

model = Sequential()
model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(Xtrain_scaled.shape[1], 1)))
model.add(LSTM(50, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])

model.summary()

model.fit(Xtrain_scaled.reshape(Xtrain_scaled.shape[0], Xtrain_scaled.shape[1], 1), ytrain_scaled, epochs=30, batch_size=32)

Xtrain_scaled.shape

"""Optimized  lstm"""

# Train LSTM model
model.fit(Xtrain_scaled.reshape(Xtrain_scaled.shape[0],Xtrain_scaled.shape[1],1),ytrain_scaled,epochs=100,batch_size=32,validation_split=0.1,verbose=1,validation_data=(Xtest_scaled.reshape(Xtest_scaled.shape[0],Xtest_scaled.shape[1],1),ytest_scaled))

model.evaluate(Xtest_scaled.reshape(Xtest_scaled.shape[0],Xtest_scaled.shape[1],1),ytest_scaled)

"""making prediction"""

lstm_predScaled = model.predict(Xtest_scaled.reshape(Xtest_scaled.shape[0],Xtest_scaled.shape[1],1))
lstm_pred = scaler_y.inverse_transform(lstm_predScaled)

# Plotting actual vs predicted prices
plt.figure(figsize=(10, 5))
plt.scatter(ytest, lstm_pred, alpha=0.5)
plt.plot([min(ytest), max(ytest)], [min(ytest), max(ytest)], 'r')
plt.title('Actual vs Predicted Prices')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.show()

"""CNN"""

from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten

cnn_model = Sequential()
cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(Xtrain_scaled.shape[1], 1)))
cnn_model.add(MaxPooling1D(pool_size=2))
cnn_model.add(Flatten())
cnn_model.add(Dense(1, activation='linear'))

cnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])

cnn_model.summary()

cnn_model.fit(Xtrain_scaled.reshape(Xtrain_scaled.shape[0], Xtrain_scaled.shape[1], 1), ytrain_scaled, epochs=100, batch_size=32, validation_split=0.1, verbose=1, validation_data=(Xtest_scaled.reshape(Xtest_scaled.shape[0], Xtest_scaled.shape[1], 1), ytest_scaled))

cnn_model.evaluate(Xtest_scaled.reshape(Xtest_scaled.shape[0],Xtest_scaled.shape[1],1),ytest_scaled)

cnn_pred = cnn_model.predict(Xtest_scaled.reshape(Xtest_scaled.shape[0],Xtest_scaled.shape[1],1))

#Plotting predicted and actual prices
plt.figure(figsize=(10,5))
plt.scatter(ytest, cnn_pred, alpha=0.5)
plt.plot([min(ytest), max(ytest)], [min(ytest), max(ytest)], 'r')
plt.title('Actual vs Predicted Prices')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.show()

import joblib
joblib.dump(xgb_model, "base_model.sav")

data = {}
feature_values = ['Ext Bag', 140, '15/3/2013', 'Horticulture', 'Rice', 2, 140, 15, 3, 2013]

for feature_name, feature_value in zip(feature_names, feature_values):
    data[feature_name] = feature_value
data

!pip install requests

import numpy as np
from sklearn.preprocessing import OneHotEncoder
from datetime import datetime

#function to get user input
def get_user_input():
    feature_values = {}
    feature_values['Produce_Variety'] = input("Enter Produce Variety (e.g., Horticulture): ")
    feature_values['Commodity_Type'] = input("Enter Commodity Type (e.g., Tomatoes): ")
    feature_values['Unit'] = input("Enter Unit (e.g., Lg Box): ")
    feature_values['Date'] = input("Enter Date (DD/MM/YYYY): ")
    feature_values['package_weight(Kg)'] = float(input("Enter Package Weight in Kg: "))
    #feature_values['Price'] = float(input("Enter the Total commodity Price in Kenyan Shillings (Ksh): "))


    # Convert the input date to a Unix timestamp
    date_obj = datetime.strptime(feature_values['Date'], '%d/%m/%Y')
    feature_values['Date_Timestamp'] = int(date_obj.timestamp())
    feature_values['Day'] = date_obj.day
    feature_values['Month'] = date_obj.month
    feature_values['Year'] = date_obj.year
    feature_values.pop('Date')
    feature_values.pop('Date_Timestamp')


    return feature_values

#getting user input
user_features = get_user_input()


#Separate numerical and categorical features
numerical_features = [
    user_features['package_weight(Kg)'],
    user_features['Day'],
    user_features['Month'],
    user_features['Year'],
    # user_features['Date_Timestamp']
    #user_features['Price']
]

categorical_features = [
    user_features['Produce_Variety'],
    user_features['Commodity_Type'],
    user_features['Unit']
]

#encode categorical features
encoder = OneHotEncoder(handle_unknown='ignore')
encoded_categorical = encoder.fit_transform([categorical_features]).toarray()
input_data = np.concatenate((numerical_features, encoded_categorical.flatten()))
input_data = np.array(input_data).reshape(1, -1)

# Scale input data
scaler = StandardScaler()
input_data = scaler.fit_transform(input_data)

# Predict the price
predicted_price = best_rf.predict(input_data) * 100
print(f"Predicted Price: {predicted_price}")

import datetime
from datetime import datetime
feature_values = {'Volume_in_Kgs': 126, 'Type_of_Commodity':5, 'Package_Type':2, 'package_weight(Kg)':126, 'Price':2205, 'Produce_Variety': 'Horticulture', 'Commodity_Type': 'Tomatoes', 'Unit': 'Lg Box', 'Date': '01/01/2023'}
numeric_fields = ['Volume_in_Kgs', 'Type_of_Commodity', 'Package_Type', 'package_weight(Kg)', 'Price']
for key in numeric_fields:
  feature_values[key] = float(feature_values[key])
  print(feature_values[key])

categorical = ['Produce_Variety', 'Commodity_Type', 'Unit']
for key in categorical:
    feature_values[key] = str(feature_values[key])
    print(feature_values[key])
#  ['Produce_Variety', 'Commodity_Type', 'Unit', 'Volume_in_Kgs', 'Date', 'Type_of_Commodity', 'Package_Type', 'package_weight(Kg)', 'Price']:


# Convert the input date to a Unix timestamp
date_obj = datetime.strptime(feature_values['Date'], '%d/%m/%Y')
# feature_values['Date_Timestamp'] = int(date_obj.timestamp())
feature_values['Day'] = date_obj.day
feature_values['Month'] = date_obj.month
feature_values['Year'] = date_obj.year
feature_values.pop('Date')

feature_values

import numpy as np
import requests
import json
from sklearn.preprocessing import OneHotEncoder
from datetime import datetime
from sklearn.preprocessing import StandardScaler

def user_input_transformer(features):

    # Separate numerical and categorical features
    numerical_features = [
        features['Volume_in_Kgs'],
        features['Type_of_Commodity'],
        features['Package_Type'],
        features['package_weight(Kg)'],
        features['Day'],
        features['Month'],
        features['Year'],
        features['Price']
    ]

    categorical_features = [
        features['Produce_Variety'],
        features['Commodity_Type'],
        features['Unit']
    ]

    # One-hot encode categorical features
    encoder = OneHotEncoder(handle_unknown='ignore')
    encoded_categorical = encoder.fit_transform([categorical_features]).toarray()

    # Combine numerical and encoded categorical features
    input_data = np.concatenate((numerical_features, encoded_categorical.flatten()))

    # Reshape the input data to a 2D array
    input_data = np.array(input_data).reshape(1, -1)
    return input_data

user_input_transformer(feature_values)

import joblib